{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee715213",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda6dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from src.model_evaluation import load_model, evaluate_model, generate_text\n",
    "from src.model_training import RNNLanguageModel, LSTMLanguageModel\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eaec17",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53afb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d781bbf",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.normpath(os.path.join(\"data\", \"word_tokenisation_reuters_data.pkl\")), \"rb\") as f:\n",
    "    word_tokenised_numericalised_docs = pickle.load(f)\n",
    "\n",
    "with open(os.path.normpath(os.path.join(\"data\", \"word_tokenisation_reuters_train_vocab.pkl\")), \"rb\") as f:\n",
    "    word_tokenisation_train_vocab = pickle.load(f)\n",
    "\n",
    "with open(os.path.normpath(os.path.join(\"data\", \"subword_tokenisation_reuters_data.pkl\")), \"rb\") as f:\n",
    "    subword_tokenised_numericalised_docs = pickle.load(f)\n",
    "\n",
    "with open(os.path.normpath(os.path.join(\"data\", \"subword_tokenisation_reuters_train_vocab.pkl\")), \"rb\") as f:\n",
    "    subword_tokenisation_train_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cd6d5",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.0\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5666e8f",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5805cb3",
   "metadata": {},
   "source": [
    "### Word Tokenisation, Context Length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "rnn = RNNLanguageModel(\n",
    "    vocab_size=len(word_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=word_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "rnn_word_tokenised_context_16_filename = \"\"\n",
    "\n",
    "rnn_word_tokenised_context_16 = load_model(model_initialised=rnn, filename=rnn_word_tokenised_context_16_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8613100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "rnn_word_tokenised_context_16_cross_entropy_loss = evaluate_model(\n",
    "    model=rnn_word_tokenised_context_16,\n",
    "    converted_tokenised_docs=word_tokenised_numericalised_docs,\n",
    "    seq_len=16,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=word_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(word_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {rnn_word_tokenised_context_16_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "rnn_word_tokenised_context_16_perplexity = math.exp(rnn_word_tokenised_context_16_cross_entropy_loss)\n",
    "print(f\"Perplexity: {rnn_word_tokenised_context_16_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23963e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "rnn_word_tokenised_context_16_generated_sequence = generate_text(\n",
    "    model=rnn_word_tokenised_context_16,\n",
    "    train_vocab=word_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(rnn_word_tokenised_context_16_generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68565712",
   "metadata": {},
   "source": [
    "### Word Tokenisation, Context Length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f5931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "rnn = RNNLanguageModel(\n",
    "    vocab_size=len(word_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=word_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "rnn_word_tokenised_context_32_filename = \"\"\n",
    "\n",
    "rnn_word_tokenised_context_32 = load_model(model_initialised=rnn, filename=rnn_word_tokenised_context_32_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "rnn_word_tokenised_context_32_cross_entropy_loss = evaluate_model(\n",
    "    model=rnn_word_tokenised_context_32,\n",
    "    converted_tokenised_docs=word_tokenised_numericalised_docs,\n",
    "    seq_len=32,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=word_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(word_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {rnn_word_tokenised_context_32_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "rnn_word_tokenised_context_32_perplexity = math.exp(rnn_word_tokenised_context_32_cross_entropy_loss)\n",
    "print(f\"Perplexity: {rnn_word_tokenised_context_32_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d75c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "rnn_word_tokenised_context_32_generated_sequence = generate_text(\n",
    "    model=rnn_word_tokenised_context_32,\n",
    "    train_vocab=word_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(rnn_word_tokenised_context_32_generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8829422c",
   "metadata": {},
   "source": [
    "### Subword Tokenisation, Context Length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "rnn = RNNLanguageModel(\n",
    "    vocab_size=len(subword_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=subword_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "rnn_subword_tokenised_context_16_filename = \"\"\n",
    "\n",
    "rnn_subword_tokenised_context_16 = load_model(model_initialised=rnn, filename=rnn_subword_tokenised_context_16_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8a09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "rnn_subword_tokenised_context_16_cross_entropy_loss = evaluate_model(\n",
    "    model=rnn_subword_tokenised_context_16,\n",
    "    converted_tokenised_docs=subword_tokenised_numericalised_docs,\n",
    "    seq_len=16,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=subword_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(subword_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {rnn_subword_tokenised_context_16_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "rnn_subword_tokenised_context_16_perplexity = math.exp(rnn_subword_tokenised_context_16_cross_entropy_loss)\n",
    "print(f\"Perplexity: {rnn_subword_tokenised_context_16_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517a572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "rnn_subword_tokenised_context_16_generated_sequence = generate_text(\n",
    "    model=rnn_subword_tokenised_context_16,\n",
    "    train_vocab=subword_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(rnn_subword_tokenised_context_16_generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9117f0eb",
   "metadata": {},
   "source": [
    "### Subword Tokenisation, Context Length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "rnn = RNNLanguageModel(\n",
    "    vocab_size=len(subword_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=subword_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "rnn_subword_tokenised_context_32_filename = \"\"\n",
    "\n",
    "rnn_subword_tokenised_context_32 = load_model(model_initialised=rnn, filename=rnn_subword_tokenised_context_32_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "rnn_subword_tokenised_context_32_cross_entropy_loss = evaluate_model(\n",
    "    model=rnn_subword_tokenised_context_32,\n",
    "    converted_tokenised_docs=subword_tokenised_numericalised_docs,\n",
    "    seq_len=32,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=subword_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(subword_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {rnn_subword_tokenised_context_32_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "rnn_subword_tokenised_context_32_perplexity = math.exp(rnn_subword_tokenised_context_32_cross_entropy_loss)\n",
    "print(f\"Perplexity: {rnn_subword_tokenised_context_32_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "rnn_subword_tokenised_context_32_generated_sequence = generate_text(\n",
    "    model=rnn_subword_tokenised_context_32,\n",
    "    train_vocab=subword_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(rnn_subword_tokenised_context_32_generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d6a5ed",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753e7b75",
   "metadata": {},
   "source": [
    "### Word Tokenisation, Context Length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78807a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "lstm = LSTMLanguageModel(\n",
    "    vocab_size=len(word_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=word_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "lstm_word_tokenised_context_16_filename = \"\"\n",
    "\n",
    "lstm_word_tokenised_context_16 = load_model(model_initialised=lstm, filename=lstm_word_tokenised_context_16_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59597b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "lstm_word_tokenised_context_16_cross_entropy_loss = evaluate_model(\n",
    "    model=lstm_word_tokenised_context_16,\n",
    "    converted_tokenised_docs=word_tokenised_numericalised_docs,\n",
    "    seq_len=16,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=word_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(word_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {lstm_word_tokenised_context_16_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "lstm_word_tokenised_context_16_perplexity = math.exp(lstm_word_tokenised_context_16_cross_entropy_loss)\n",
    "print(f\"Perplexity: {lstm_word_tokenised_context_16_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3d2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "lstm_word_tokenised_context_16_generated_sequence = generate_text(\n",
    "    model=lstm_word_tokenised_context_16,\n",
    "    train_vocab=word_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(lstm_word_tokenised_context_16_generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2fe55d",
   "metadata": {},
   "source": [
    "### Word Tokenisation, Context Length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c4df57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "lstm = LSTMLanguageModel(\n",
    "    vocab_size=len(word_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=word_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "lstm_word_tokenised_context_32_filename = \"\"\n",
    "\n",
    "lstm_word_tokenised_context_32 = load_model(model_initialised=lstm, filename=lstm_word_tokenised_context_32_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "lstm_word_tokenised_context_32_cross_entropy_loss = evaluate_model(\n",
    "    model=lstm_word_tokenised_context_32,\n",
    "    converted_tokenised_docs=word_tokenised_numericalised_docs,\n",
    "    seq_len=32,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=word_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(word_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {lstm_word_tokenised_context_32_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "lstm_word_tokenised_context_32_perplexity = math.exp(lstm_word_tokenised_context_32_cross_entropy_loss)\n",
    "print(f\"Perplexity: {lstm_word_tokenised_context_32_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "lstm_word_tokenised_context_32_generated_sequence = generate_text(\n",
    "    model=lstm_word_tokenised_context_32,\n",
    "    train_vocab=word_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(lstm_word_tokenised_context_32_generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7980feb7",
   "metadata": {},
   "source": [
    "### Subword Tokenisation, Context Length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "lstm = LSTMLanguageModel(\n",
    "    vocab_size=len(subword_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=subword_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "lstm_subword_tokenised_context_16_filename = \"\"\n",
    "\n",
    "lstm_subword_tokenised_context_16 = load_model(model_initialised=lstm, filename=lstm_subword_tokenised_context_16_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61582779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "lstm_subword_tokenised_context_16_cross_entropy_loss = evaluate_model(\n",
    "    model=lstm_subword_tokenised_context_16,\n",
    "    converted_tokenised_docs=subword_tokenised_numericalised_docs,\n",
    "    seq_len=16,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=subword_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(subword_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {lstm_subword_tokenised_context_16_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "lstm_subword_tokenised_context_16_perplexity = math.exp(lstm_subword_tokenised_context_16_cross_entropy_loss)\n",
    "print(f\"Perplexity: {lstm_subword_tokenised_context_16_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9def2bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "lstm_subword_tokenised_context_16_generated_sequence = generate_text(\n",
    "    model=lstm_subword_tokenised_context_16,\n",
    "    train_vocab=subword_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(lstm_subword_tokenised_context_16_generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f3847b",
   "metadata": {},
   "source": [
    "### Subword Tokenisation, Context Length = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e8d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "lstm = LSTMLanguageModel(\n",
    "    vocab_size=len(subword_tokenisation_train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=subword_tokenisation_train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "\n",
    "lstm_subword_tokenised_context_32_filename = \"\"\n",
    "\n",
    "lstm_subword_tokenised_context_32 = load_model(model_initialised=lstm, filename=lstm_subword_tokenised_context_32_filename, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b89100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss\n",
    "lstm_subword_tokenised_context_32_cross_entropy_loss = evaluate_model(\n",
    "    model=lstm_subword_tokenised_context_32,\n",
    "    converted_tokenised_docs=subword_tokenised_numericalised_docs,\n",
    "    seq_len=32,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    criterion=nn.CrossEntropyLoss(ignore_index=subword_tokenisation_train_vocab[\"<pad>\"])\n",
    "    device=device,\n",
    "    vocab_size=len(subword_tokenisation_train_vocab)\n",
    ")\n",
    "print(f\"Cross Entropy Loss: {lstm_subword_tokenised_context_32_cross_entropy_loss}\")\n",
    "\n",
    "# Perplexity\n",
    "lstm_subword_tokenised_context_32_perplexity = math.exp(lstm_subword_tokenised_context_32_cross_entropy_loss)\n",
    "print(f\"Perplexity: {lstm_subword_tokenised_context_32_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb09824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "starting_sequence = [\"Today\"]\n",
    "\n",
    "lstm_subword_tokenised_context_32_generated_sequence = generate_text(\n",
    "    model=lstm_subword_tokenised_context_32,\n",
    "    train_vocab=subword_tokenisation_train_vocab,\n",
    "    start_seq=starting_sequence,\n",
    "    temperature=1.0\n",
    ")\n",
    "print(f\"Generated Sequence from {starting_sequence}:\")\n",
    "print(lstm_subword_tokenised_context_32_generated_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
