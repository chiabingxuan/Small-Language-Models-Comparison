{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97c0f2d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e2bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bxchi\\OneDrive\\Desktop\\Stuff\\Year 4 Sem 1\\DSA4213\\Assignments\\Assignment 2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.model_training import RNNLanguageModel, LSTMLanguageModel, train, plot_and_save_training_metrics\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf9dec6",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99b84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.enabled = False\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6f27c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8993a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.normpath(os.path.join(\"data\", \"word_tokenisation_reuters_data.pkl\")), \"rb\") as f:\n",
    "    word_tokenised_numericalised_docs = pickle.load(f)\n",
    "\n",
    "with open(os.path.normpath(os.path.join(\"data\", \"word_tokenisation_reuters_train_vocab.pkl\")), \"rb\") as f:\n",
    "    word_tokenisation_train_vocab = pickle.load(f)\n",
    "\n",
    "with open(os.path.normpath(os.path.join(\"data\", \"subword_tokenisation_reuters_data.pkl\")), \"rb\") as f:\n",
    "    subword_tokenised_numericalised_docs = pickle.load(f)\n",
    "\n",
    "with open(os.path.normpath(os.path.join(\"data\", \"subword_tokenisation_reuters_train_vocab.pkl\")), \"rb\") as f:\n",
    "    subword_tokenisation_train_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d4f0b2",
   "metadata": {},
   "source": [
    "## Set Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f41172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.0\n",
    "USE_WORD_TOKENISATION = True    # Choose either word tokenisation or subword tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469eddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_WORD_TOKENISATION:\n",
    "    converted_tokenised_docs = word_tokenised_numericalised_docs\n",
    "    train_vocab = word_tokenisation_train_vocab\n",
    "else:\n",
    "    converted_tokenised_docs = subword_tokenised_numericalised_docs\n",
    "    train_vocab = subword_tokenisation_train_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2640682f",
   "metadata": {},
   "source": [
    "## Set Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 32\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "GRADIENT_CLIPPING_MAX_NORM = 1.0\n",
    "PATIENCE = 5       # For early stopping. Represents max number of consecutive epochs where val loss does not improve, before early stopping is triggered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad0738",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d167e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "rnn = RNNLanguageModel(\n",
    "    vocab_size=len(train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "rnn_save_name = f\"rnn_word_tokens_{USE_WORD_TOKENISATION}_context_{SEQUENCE_LENGTH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7603/7603 [36:50<00:00,  3.44it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 3.6771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [01:36<00:00,  8.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 8.1882\n",
      "Best val loss has improved. Counter: 0 | Best val loss: 8.188167695570918\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7603/7603 [1:00:01<00:00,  2.11it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 824/824 [01:12<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 9.6689\n",
      "Best val loss did not improve. Counter: 1 | Best val loss: 8.188167695570918\n",
      "\n",
      "Epoch 2/9\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 4724/7603 [42:32<16:17,  2.94it/s]     "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "rnn_trained, rnn_train_loss_history, rnn_val_loss_history = train(\n",
    "    model=rnn,\n",
    "    converted_tokenised_docs=converted_tokenised_docs,\n",
    "    train_vocab=train_vocab,\n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    grad_clipping_max_norm=GRADIENT_CLIPPING_MAX_NORM,\n",
    "    patience=PATIENCE,\n",
    "    device=device,\n",
    "    save_name=rnn_save_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd92899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View and save loss curves\n",
    "plot_and_save_training_metrics(train_loss_history=rnn_train_loss_history, val_loss_history=rnn_val_loss_history, save_name=rnn_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf60d7d",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc00fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model\n",
    "lstm = LSTMLanguageModel(\n",
    "    vocab_size=len(train_vocab),\n",
    "    embed_size=EMBEDDING_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_idx=train_vocab[\"<pad>\"]\n",
    ").to(device)\n",
    "lstm_save_name = f\"lstm_word_tokens_{USE_WORD_TOKENISATION}_context_{SEQUENCE_LENGTH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c468e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lstm_trained, lstm_train_loss_history, lstm_val_loss_history = train(\n",
    "    model=lstm,\n",
    "    converted_tokenised_docs=converted_tokenised_docs,\n",
    "    train_vocab=train_vocab,\n",
    "    seq_len=SEQUENCE_LENGTH,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    lr=LEARNING_RATE,\n",
    "    grad_clipping_max_norm=GRADIENT_CLIPPING_MAX_NORM,\n",
    "    patience=PATIENCE,\n",
    "    device=device,\n",
    "    save_name=lstm_save_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5d681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View and save loss curves\n",
    "plot_and_save_training_metrics(train_loss_history=lstm_train_loss_history, val_loss_history=lstm_val_loss_history, save_name=lstm_save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
